{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Constants** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 1024\n",
    "output_dim = 4\n",
    "learning_rate = 0.001\n",
    "word_vector_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '..\\model'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DataLoader.__init__() missing 1 required positional argument: 'dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_data_loader \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39;49mDataLoader() \u001b[39m#TODO\u001b[39;00m\n\u001b[0;32m      2\u001b[0m test_data_loader \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mDataLoader() \u001b[39m#TODO\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: DataLoader.__init__() missing 1 required positional argument: 'dataset'"
     ]
    }
   ],
   "source": [
    "train_data_loader = data.DataLoader() #TODO\n",
    "test_data_loader = data.DataLoader() #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dummy data\n",
    "dummy_labels = {\n",
    "        'Dynamic API Resolution': 'Adversaries may obfuscate then dynamically resolve API functions called by their malware in order to conceal malicious functionalities and impair defensive analysis. Malware commonly uses various Native API functions provided by the OS to perform various tasks such as those involving processes, files, and other system artifacts.',\n",
    "        'HTML Smuggling':'Adversaries may smuggle data and files past content filters by hiding malicious payloads inside of seemingly benign HTML files. HTML documents can store large binary objects known as JavaScript Blobs (immutable data that represents raw bytes) that can later be constructed into file-like objects. Data may also be stored in Data URLs, which enable embedding media type or MIME files inline of HTML documents. HTML5 also introduced a download attribute that may be used to initiate file downloads.',\n",
    "        'Network Denial of Service':'Adversaries may perform Network Denial of Service (DoS) attacks to degrade or block the availability of targeted resources to users. Network DoS can be performed by exhausting the network bandwidth services rely on. Example resources include specific websites, email services, DNS, and web-based applications. Adversaries have been observed conducting network DoS attacks for political purposes and to support other malicious activities, including distraction, hacktivism, and extortion.',\n",
    "        'Network Sniffing':'Adversaries may sniff network traffic to capture information about an environment, including authentication material passed over the network. Network sniffing refers to using the network interface on a system to monitor or capture information sent over a wired or wireless connection. An adversary may place a network interface into promiscuous mode to passively access data in transit over the network, or use span ports to capture a larger amount of data.'\n",
    "}   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Word2Vec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import gensim.downloader as api\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [description.split() for label, description in dummy_labels.items()]\n",
    "corpus = []\n",
    "for label, description in dummy_labels.items():\n",
    "    words = [word.translate(str.maketrans('', '', string.punctuation)) for word in (f\"{label} {description}\").split()]\n",
    "    corpus.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus = api.load('text8')\n",
    "gensim_model = Word2Vec(corpus, vector_size=word_vector_size, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelsEmbeddings():\n",
    "    def __init__(self, gensim_model: Word2Vec):\n",
    "        self.model = gensim_model\n",
    "\n",
    "    def fix_vectors_sizes(self, vectors: list) -> list:\n",
    "        fixed_vectors = []\n",
    "        max_size = max([len(v) for v in vectors])\n",
    "        for vector in vectors:\n",
    "            size_diff = max_size - len(vector)\n",
    "            vector.extend([[0] * word_vector_size] * size_diff)\n",
    "            fixed_vectors.append(vector)\n",
    "        return fixed_vectors\n",
    "\n",
    "    def generate_vectors(self, labels: dict):\n",
    "        vectors = []\n",
    "        for label, _ in labels.items():\n",
    "            description_vector = []\n",
    "            words = [word.translate(str.maketrans('', '', string.punctuation)) for word in label.split()]\n",
    "            for word in words:\n",
    "                if word in self.model.wv.index_to_key:\n",
    "                    description_vector.append(self.model.wv[word])\n",
    "                else:\n",
    "                    description_vector.append(word_vector_size * [0])\n",
    "            vectors.append(description_vector)\n",
    "        return self.fix_vectors_sizes(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_embeddings = LabelsEmbeddings(gensim_model=gensim_model)\n",
    "labels_vectors = labels_embeddings.generate_vectors(labels=dummy_labels)\n",
    "#TODO need to pad?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 4, 4, 4]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(v) for v in labels_vectors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vector in labels_vectors:\n",
    "    for word in vector:\n",
    "        assert len(word) == word_vector_size\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Map layer**\n",
    "Last layer of the model should be a map between incident embeddings and labels embeddings - we want to map given input data to the most corresponding Word2Vec label vector. To do this we have to initialize weights of this layer and freeze them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_layer_init(shape: int, w2c_vectors: list) -> torch.Tensor:\n",
    "    vectors = np.asarray(w2c_vectors, dtype=np.float)\n",
    "    return torch.from_numpy(vectors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Neural Network for Network Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetNet(nn.Module):\n",
    "    def __init__(self, input_dim: int, words_embeddings_dim: int, output_dim: int, labels_vectors: torch.Tensor):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, 512)\n",
    "        self.linear2 = nn.Linear(512, 256)\n",
    "        self.linear3 = nn.Linear(256, words_embeddings_dim)\n",
    "        self.linear4 = nn.Linear(words_embeddings_dim, output_dim)\n",
    "        self.ReLU = nn.ReLU()\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "        # weight initialize\n",
    "        self.linear4.weight.data = map_layer_init(shape=words_embeddings_dim, w2c_vectors=labels_vectors)\n",
    "        # freeze layer weights\n",
    "        self.linear4.weight.requires_grad = False\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ReLU(self.linear1(x))\n",
    "        x = self.ReLU(self.linear2(x))\n",
    "        x = self.ReLU(self.linear3(x))\n",
    "        x = self.softmax(self.linear4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "map_layer_init() missing 1 required positional argument: 'w2c_vectors'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m NetNet(\n\u001b[0;32m      2\u001b[0m     input_dim\u001b[39m=\u001b[39;49minput_dim, \n\u001b[0;32m      3\u001b[0m     words_embeddings_dim\u001b[39m=\u001b[39;49mword_vector_size, \n\u001b[0;32m      4\u001b[0m     output_dim\u001b[39m=\u001b[39;49moutput_dim, \n\u001b[0;32m      5\u001b[0m     labels_vectors\u001b[39m=\u001b[39;49mlabels_vectors\n\u001b[0;32m      6\u001b[0m )\n",
      "Cell \u001b[1;32mIn[15], line 12\u001b[0m, in \u001b[0;36mNetNet.__init__\u001b[1;34m(self, input_dim, words_embeddings_dim, output_dim, labels_vectors)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msoftmax \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSoftmax()\n\u001b[0;32m     11\u001b[0m \u001b[39m# weight initialize\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear4\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m map_layer_init(labels_vectors)\n\u001b[0;32m     13\u001b[0m \u001b[39m# freeze layer weights\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear4\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mrequires_grad \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: map_layer_init() missing 1 required positional argument: 'w2c_vectors'"
     ]
    }
   ],
   "source": [
    "model = NetNet(\n",
    "    input_dim=input_dim, \n",
    "    words_embeddings_dim=word_vector_size, \n",
    "    output_dim=output_dim, \n",
    "    labels_vectors=labels_vectors\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Training** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model: NetNet, epochs: int, data_loader: data.DataLoader, loss_fn: nn.MSELoss):\n",
    "        model.train()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for inputs, labels in data_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                loss.backward()\n",
    "                model.optim.step()\n",
    "                model.optim.zero_grad()\n",
    "\n",
    "            if epoch % 10 == 1:\n",
    "                print(f\"Epoch: {epoch}, loss: {loss.item():.3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "model.optim = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "train_model(model=model, epochs=150, data_loader=train_data_loader, loss_fn=loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Evaluation** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model: NetNet, data_loader: data.DataLoader):\n",
    "        model.eval()\n",
    "        true_predictions, predicitons_amount = 0., 0.\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in data_loader:\n",
    "                predictions = model(inputs)\n",
    "                predictions = predictions.squeeze(dim=1)\n",
    "                true_predictions += (predictions == labels).sum()\n",
    "                predicitons_amount += labels.shape[0]\n",
    "\n",
    "            accuracy = 100.0 * true_predictions / predicitons_amount\n",
    "        \n",
    "        print(f\"Accuracy of the model: {accuracy:4.2f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Removing last layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
