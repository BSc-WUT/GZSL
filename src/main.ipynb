{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Load and Prepare Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from dataset.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "import dataset\n",
    "from dataset import parse_dataset\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\n",
    "    'Benign',\n",
    "    'Bot',\n",
    "    'Brute Force -XSS',\n",
    "    'DDOS attack-HOIC',\n",
    "    'DDOS attack-LOIC-UDP',\n",
    "    'DDoS attacks-LOIC-HTTP',\n",
    "    'DoS attacks-GoldenEye',\n",
    "    'DoS attacks-Hulk',\n",
    "    'DoS attacks-Slowloris',\n",
    "    'FTP-BruteForce',\n",
    "    'SSH-Bruteforce',\n",
    "    'Label',\n",
    "    'Brute Force -Web',\n",
    "    'DoS attacks-SlowHTTPTest',\n",
    "    'Infilteration',\n",
    "    'SQL Injection'                      \n",
    "]\n",
    "\n",
    "labels_emb = {}\n",
    "for i, label in enumerate(labels):\n",
    "    labels_emb[label] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Train data classes**\n",
    "- Benign                      \n",
    "- Bot                                           \n",
    "- Brute Force -XSS                 \n",
    "- DDOS attack-HOIC              \n",
    "- DDOS attack-LOIC-UDP            \n",
    "- DDoS attacks-LOIC-HTTP      \n",
    "- DoS attacks-GoldenEye          \n",
    "- DoS attacks-Hulk                    \n",
    "- DoS attacks-Slowloris          \n",
    "- FTP-BruteForce                                \n",
    "- Label                                                \n",
    "- SSH-Bruteforce \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = [\n",
    "    'Bot',\n",
    "    'Brute Force -XSS',\n",
    "    'DDOS attack-HOIC',\n",
    "    'DDOS attack-LOIC-UDP',\n",
    "    'DDoS attacks-LOIC-HTTP',\n",
    "    'DoS attacks-GoldenEye',\n",
    "    'DoS attacks-Hulk',\n",
    "    'DoS attacks-Slowloris',\n",
    "    'FTP-BruteForce',\n",
    "    'SSH-Bruteforce',\n",
    "    'Label',\n",
    "                          \n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Test data will consists of classes**\n",
    "- Brute Force -Web                 \n",
    "- DoS attacks-SlowHTTPTest      \n",
    "- Infilteration                                              \n",
    "- SQL Injection                     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = [label for label in labels if label not in train_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['Timestamp', 'Flow ID', 'Src IP', 'Src Port', 'Dst IP', 'Dst Port']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PICKLE_PATH = '../data/pickle/'\n",
    "TRAIN_PICKLE = 'train_dataset.pkl'\n",
    "TEST_PICKLE = 'test_dataset.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:3: DtypeWarning: Columns (0,1,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78) have mixed types. Specify dtype option on import or set low_memory=False.\n"
     ]
    }
   ],
   "source": [
    "train_pickle_path = os.path.join(PICKLE_PATH, TRAIN_PICKLE)\n",
    "test_pickle_path = os.path.join(PICKLE_PATH, TEST_PICKLE)\n",
    "\n",
    "if TRAIN_PICKLE not in os.listdir(PICKLE_PATH) and TEST_PICKLE not in os.listdir(PICKLE_PATH):\n",
    "    ids_dataset = dataset.DatasetIDS2018(csv_file_name='../data/raw/small_merge_data.csv')\n",
    "    train = ids_dataset.get_data_by_labels(labels=train_labels)\n",
    "    train_dataset = parse_dataset(dataset=train, columns_to_drop=columns_to_drop, fixed_type=float, labels_emb=labels_emb, labels_column_name='Label')\n",
    "    train_dataset.to_pickle(train_pickle_path)\n",
    "    test = ids_dataset.get_data_by_labels(labels=test_labels)\n",
    "    test_dataset = parse_dataset(dataset=test, columns_to_drop=columns_to_drop, fixed_type=float, labels_emb=labels_emb, labels_column_name='Label')\n",
    "    test_dataset.to_pickle(test_pickle_path)\n",
    "else:\n",
    "    train_dataset = pd.read_pickle(train_pickle_path)\n",
    "    test_dataset = pd.read_pickle(test_pickle_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = data.TensorDataset(torch.from_numpy(test_dataset.values).float(),torch.from_numpy(test_dataset.values[:,-1].astype(float)).float())\n",
    "train_dataset = data.TensorDataset(torch.from_numpy(train_dataset.values).float(),torch.from_numpy(train_dataset.values[:,-1].astype(float)).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True) \n",
    "test_data_loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[6.0000e+00, 1.3970e+03, 2.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "          3.0000e+00],\n",
       "         [6.0000e+00, 5.1800e+02, 2.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "          1.0000e+00],\n",
       "         [6.0000e+00, 3.0398e+06, 3.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "          8.0000e+00],\n",
       "         ...,\n",
       "         [6.0000e+00, 1.3403e+04, 3.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "          3.0000e+00],\n",
       "         [6.0000e+00, 1.3099e+04, 2.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "          7.0000e+00],\n",
       "         [6.0000e+00, 4.4244e+04, 2.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "          7.0000e+00]]),\n",
       " tensor([ 3.,  1.,  8.,  7.,  1.,  1.,  3.,  9.,  9.,  7.,  1.,  3.,  9.,  3.,\n",
       "          3.,  3.,  1.,  1.,  6.,  7.,  7.,  3., 10.,  1.,  6.,  7.,  6.,  9.,\n",
       "          7.,  3.,  7.,  7.])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[6.0000e+00, 2.1464e+04, 2.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00],\n",
       "         [6.0000e+00, 5.8772e+05, 5.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00],\n",
       "         [6.0000e+00, 2.5160e+06, 8.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00],\n",
       "         ...,\n",
       "         [6.0000e+00, 2.0000e+00, 1.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "          1.3000e+01],\n",
       "         [6.0000e+00, 3.3000e+01, 1.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00],\n",
       "         [6.0000e+00, 7.6000e+01, 1.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00]]),\n",
       " tensor([ 0.,  0.,  0.,  0.,  0., 14.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 13.,  0., 13.,\n",
       "          0., 13.,  0.,  0.])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(test_data_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vector_size = 100\n",
    "input_dim = len(train_dataset[0][0])\n",
    "output_dim = len(labels)\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Word2Vec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import gensim.downloader as api\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for label in labels:\n",
    "    words = [word.translate(str.maketrans('', '', string.punctuation)) for word in label.split()]\n",
    "    corpus.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_model = Word2Vec(corpus, vector_size=word_vector_size, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelsEmbeddings():\n",
    "    def __init__(self, gensim_model: Word2Vec):\n",
    "        self.model = gensim_model\n",
    "\n",
    "    def fix_vectors_sizes(self, vectors: list) -> list:\n",
    "        fixed_vectors = []\n",
    "        max_size = max([len(v) for v in vectors])\n",
    "        for vector in vectors:\n",
    "            size_diff = max_size - len(vector)\n",
    "            vector.extend([[0] * word_vector_size] * size_diff)\n",
    "            fixed_vectors.append(vector)\n",
    "        return fixed_vectors\n",
    "\n",
    "    def generate_vectors(self, labels: dict):\n",
    "        vectors = []\n",
    "        for label in labels:\n",
    "            description_vector = []\n",
    "            words = [word.translate(str.maketrans('', '', string.punctuation)) for word in label.split()]\n",
    "            for word in words:\n",
    "                if word in self.model.wv.index_to_key:\n",
    "                    description_vector.append(self.model.wv[word])\n",
    "                else:\n",
    "                    description_vector.append(word_vector_size * [0])\n",
    "            vectors.append(description_vector)\n",
    "        return self.fix_vectors_sizes(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_embeddings = LabelsEmbeddings(gensim_model=gensim_model)\n",
    "labels_vectors = labels_embeddings.generate_vectors(labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(v) for v in labels_vectors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vector in labels_vectors:\n",
    "    for word in vector:\n",
    "        assert len(word) == word_vector_size\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Map layer**\n",
    "Last layer of the model should be a map between incident embeddings and labels embeddings - we want to map given input data to the most corresponding Word2Vec label vector. To do this we have to initialize weights of this layer and freeze them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_layer_init(w2c_vectors: list) -> torch.Tensor:\n",
    "    vectors = np.asarray(w2c_vectors, dtype=float)\n",
    "    vectors = torch.from_numpy(vectors)\n",
    "    return vectors[:, -1, :].to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: torch.Size([16, 100])\n",
      "Type: torch.float32\n"
     ]
    }
   ],
   "source": [
    "map_layer = map_layer_init(labels_vectors)\n",
    "print(f\"Size: {map_layer.size()}\\nType: {map_layer.dtype}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Neural Network for Network Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetNet(nn.Module):\n",
    "    def __init__(self, input_dim: int, words_embeddings_dim: int, output_dim: int, labels_vectors: torch.Tensor):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, 512)\n",
    "        self.linear2 = nn.Linear(512, 256)\n",
    "        self.linear3 = nn.Linear(256, words_embeddings_dim)\n",
    "        self.linear4 = nn.Linear(words_embeddings_dim, output_dim)\n",
    "        self.ReLU = nn.ReLU()\n",
    "\n",
    "        # weight initialize\n",
    "        self.linear4.weight.data = map_layer_init(w2c_vectors=labels_vectors)\n",
    "        # freeze layer weights\n",
    "        self.linear4.weight.requires_grad = False\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ReLU(self.linear1(x))\n",
    "        x = self.ReLU(self.linear2(x))\n",
    "        x = self.ReLU(self.linear3(x))\n",
    "        x = self.ReLU(self.linear4(x))\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NetNet(\n",
    "    input_dim=input_dim, \n",
    "    words_embeddings_dim=word_vector_size, \n",
    "    output_dim=output_dim, \n",
    "    labels_vectors=labels_vectors\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetNet(\n",
      "  (linear1): Linear(in_features=78, out_features=512, bias=True)\n",
      "  (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (linear3): Linear(in_features=256, out_features=100, bias=True)\n",
      "  (linear4): Linear(in_features=100, out_features=16, bias=True)\n",
      "  (ReLU): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Training** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alive_progress import alive_bar\n",
    "\n",
    "def train_model(model: NetNet, epochs: int, data_loader: data.DataLoader, loss_fn: nn.MSELoss):\n",
    "        model.train()\n",
    "        with alive_bar(epochs) as bar:\n",
    "            for epoch in range(epochs):\n",
    "                for inputs, labels in data_loader:\n",
    "                    outputs = model(inputs)\n",
    "                    loss = loss_fn(outputs, labels.to(torch.long))\n",
    "                    loss.backward()\n",
    "                    model.optim.step()\n",
    "                    model.optim.zero_grad()\n",
    "\n",
    "                print(f\"Epoch: {epoch}, loss: {loss.item():.3}\")               \n",
    "                bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on 0: Epoch: 0, loss: nan\n",
      "|████⚠︎                                   | (!) 1/10 [10%] in 3:55.9 (0.00/s) \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m loss_fn \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m      2\u001b[0m model\u001b[39m.\u001b[39moptim \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mlearning_rate)\n\u001b[1;32m----> 3\u001b[0m train_model(model\u001b[39m=\u001b[39;49mmodel, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, data_loader\u001b[39m=\u001b[39;49mtrain_data_loader, loss_fn\u001b[39m=\u001b[39;49mloss_fn)\n",
      "Cell \u001b[1;32mIn[28], line 12\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, epochs, data_loader, loss_fn)\u001b[0m\n\u001b[0;32m     10\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     11\u001b[0m     model\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mstep()\n\u001b[1;32m---> 12\u001b[0m     model\u001b[39m.\u001b[39;49moptim\u001b[39m.\u001b[39;49mzero_grad()\n\u001b[0;32m     14\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m, loss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m.\u001b[39mitem()\u001b[39m:\u001b[39;00m\u001b[39m.3\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)               \n\u001b[0;32m     15\u001b[0m bar()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\optimizer.py:456\u001b[0m, in \u001b[0;36mOptimizer.zero_grad\u001b[1;34m(self, set_to_none)\u001b[0m\n\u001b[0;32m    454\u001b[0m \u001b[39mif\u001b[39;00m foreach:\n\u001b[0;32m    455\u001b[0m     per_device_and_dtype_grads \u001b[39m=\u001b[39m defaultdict(\u001b[39mlambda\u001b[39;00m: defaultdict(\u001b[39mlist\u001b[39m))\n\u001b[1;32m--> 456\u001b[0m \u001b[39mwith\u001b[39;49;00m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mprofiler\u001b[39m.\u001b[39;49mrecord_function(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_zero_grad_profile_name):\n\u001b[0;32m    457\u001b[0m     \u001b[39mfor\u001b[39;49;00m group \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_groups:\n\u001b[0;32m    458\u001b[0m         \u001b[39mfor\u001b[39;49;00m p \u001b[39min\u001b[39;49;00m group[\u001b[39m'\u001b[39;49m\u001b[39mparams\u001b[39;49m\u001b[39m'\u001b[39;49m]:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\autograd\\profiler.py:492\u001b[0m, in \u001b[0;36mrecord_function.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    491\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__enter__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 492\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrecord \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mops\u001b[39m.\u001b[39;49mprofiler\u001b[39m.\u001b[39;49m_record_function_enter_new(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs)\n\u001b[0;32m    493\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\_ops.py:502\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    498\u001b[0m     \u001b[39m# overloading __call__ to ensure torch.ops.foo.bar()\u001b[39;00m\n\u001b[0;32m    499\u001b[0m     \u001b[39m# is still callable from JIT\u001b[39;00m\n\u001b[0;32m    500\u001b[0m     \u001b[39m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[0;32m    501\u001b[0m     \u001b[39m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[1;32m--> 502\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_op(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs \u001b[39mor\u001b[39;49;00m {})\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "model.optim = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "train_model(model=model, epochs=10, data_loader=train_data_loader, loss_fn=loss_fn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Removing last layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "zsl_model = nn.Sequential(*(list(model.children())[:3] + list(model.children())[4:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetNet(\n",
      "  (linear1): Linear(in_features=78, out_features=512, bias=True)\n",
      "  (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (linear3): Linear(in_features=256, out_features=100, bias=True)\n",
      "  (linear4): Linear(in_features=100, out_features=16, bias=True)\n",
      "  (ReLU): ReLU()\n",
      ")\n",
      "Sequential(\n",
      "  (0): Linear(in_features=78, out_features=512, bias=True)\n",
      "  (1): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (2): Linear(in_features=256, out_features=100, bias=True)\n",
      "  (3): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "print(zsl_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Evaluation** \n",
    "For this step we will calculate euclidean distance and find the vector which is the closest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_vector(vector: torch.Tensor, classes: torch.Tensor) -> tuple(str, torch.Tensor):\n",
    "    min_dist = float('inf')\n",
    "    min_dist_label = ''\n",
    "    for label, class_vector in classes:\n",
    "        dist = torch.cdist(class_vector, vector, p=2)\n",
    "        dist = dist.squeeze(dim=0)\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            min_dist_label = label\n",
    "    return min_dist_label, min_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model: NetNet, data_loader: data.DataLoader, dataset: data.Dataset):\n",
    "        model.eval()\n",
    "        true_predictions, predicitons_amount = 0., 0.\n",
    "        labels = dataset.targets\n",
    "             \n",
    "        with torch.no_grad():\n",
    "            for inputs, label in data_loader:\n",
    "                pred_input = model(inputs)\n",
    "                pred_label, dist = find_closest_vector(vector=pred_input, classes=labels)\n",
    "                predictions = predictions.squeeze(dim=1)\n",
    "                true_predictions += int(pred_label == label)\n",
    "                predicitons_amount += 1\n",
    "\n",
    "            accuracy = 100.0 * true_predictions / predicitons_amount\n",
    "        \n",
    "        print(f\"Accuracy of the model: {accuracy:4.2f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train_model(epochs=150, data_loader=[])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
