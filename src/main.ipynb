{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gqEJsry3cwoP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O09ALdY8c_bw",
        "outputId": "b1b01a1c-fe0e-422a-fa64-cee6342bf5c2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "else:\n",
        "  device = 'cpu'\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pecHc7gvfOLG",
        "outputId": "e10cf4e2-afd4-4cb9-c9f9-ee3d41e57702"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install import_ipynb"
      ],
      "metadata": {
        "id": "T4U1R_IA6J9K",
        "outputId": "1e4d302b-c564-4558-c642-95f24b6923dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting import_ipynb\n",
            "  Downloading import_ipynb-0.1.4-py3-none-any.whl (4.1 kB)\n",
            "Requirement already satisfied: IPython in /usr/local/lib/python3.10/dist-packages (from import_ipynb) (7.34.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from import_ipynb) (5.9.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from IPython->import_ipynb) (67.7.2)\n",
            "Collecting jedi>=0.16 (from IPython->import_ipynb)\n",
            "  Downloading jedi-0.19.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from IPython->import_ipynb) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from IPython->import_ipynb) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from IPython->import_ipynb) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from IPython->import_ipynb) (3.0.39)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from IPython->import_ipynb) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from IPython->import_ipynb) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from IPython->import_ipynb) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from IPython->import_ipynb) (4.8.0)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat->import_ipynb) (2.18.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->import_ipynb) (4.19.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.10/dist-packages (from nbformat->import_ipynb) (5.3.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->IPython->import_ipynb) (0.8.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->import_ipynb) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->import_ipynb) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->import_ipynb) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->import_ipynb) (0.9.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->IPython->import_ipynb) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->IPython->import_ipynb) (0.2.6)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core->nbformat->import_ipynb) (3.10.0)\n",
            "Installing collected packages: jedi, import_ipynb\n",
            "Successfully installed import_ipynb-0.1.4 jedi-0.19.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPY6CHvBcwoQ"
      },
      "source": [
        "## **Load and Prepare Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpBqQoeLcwoR",
        "outputId": "85b4997e-3448-4a3f-c6ec-537d78fe9b22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "importing Jupyter notebook from dataset.ipynb\n"
          ]
        }
      ],
      "source": [
        "import import_ipynb\n",
        "import dataset\n",
        "from dataset import parse_dataset\n",
        "import os\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "nLrMfSL3cwoR"
      },
      "outputs": [],
      "source": [
        "labels = [\n",
        "    'Benign',\n",
        "    'Bot',\n",
        "    'Brute Force -XSS',\n",
        "    'DDOS attack-HOIC',\n",
        "    'DDOS attack-LOIC-UDP',\n",
        "    'DDoS attacks-LOIC-HTTP',\n",
        "    'DoS attacks-GoldenEye',\n",
        "    'DoS attacks-Hulk',\n",
        "    'DoS attacks-Slowloris',\n",
        "    'FTP-BruteForce',\n",
        "    'SSH-Bruteforce',\n",
        "    'Label',\n",
        "    'Brute Force -Web',\n",
        "    'DoS attacks-SlowHTTPTest',\n",
        "    'Infilteration',\n",
        "    'SQL Injection'\n",
        "]\n",
        "\n",
        "labels_emb = {}\n",
        "for i, label in enumerate(labels):\n",
        "    labels_emb[label] = i"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQQ9qC8ZcwoR"
      },
      "source": [
        "### **Train data classes**\n",
        "- Benign                      \n",
        "- Bot                                           \n",
        "- Brute Force -XSS                 \n",
        "- DDOS attack-HOIC              \n",
        "- DDOS attack-LOIC-UDP            \n",
        "- DDoS attacks-LOIC-HTTP      \n",
        "- DoS attacks-GoldenEye          \n",
        "- DoS attacks-Hulk                    \n",
        "- DoS attacks-Slowloris          \n",
        "- FTP-BruteForce                                \n",
        "- Label                                                \n",
        "- SSH-Bruteforce\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "SzlSz8CWcwoS"
      },
      "outputs": [],
      "source": [
        "train_labels = [\n",
        "    'Bot',\n",
        "    'Brute Force -XSS',\n",
        "    'DDOS attack-HOIC',\n",
        "    'DDOS attack-LOIC-UDP',\n",
        "    'DDoS attacks-LOIC-HTTP',\n",
        "    'DoS attacks-GoldenEye',\n",
        "    'DoS attacks-Hulk',\n",
        "    'DoS attacks-Slowloris',\n",
        "    'FTP-BruteForce',\n",
        "    'SSH-Bruteforce',\n",
        "    'Label',\n",
        "\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCPrXDzncwoS"
      },
      "source": [
        "### **Test data will consists of classes**\n",
        "- Brute Force -Web                 \n",
        "- DoS attacks-SlowHTTPTest      \n",
        "- Infilteration                                              \n",
        "- SQL Injection                     \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4E6XRxZ3cwoS"
      },
      "outputs": [],
      "source": [
        "test_labels = [label for label in labels if label not in train_labels]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7RvkdRYecwoS"
      },
      "outputs": [],
      "source": [
        "columns_to_drop = ['Timestamp', 'Flow ID', 'Src IP', 'Src Port', 'Dst IP', 'Dst Port']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "YRX1s7ELcwoS"
      },
      "outputs": [],
      "source": [
        "PICKLE_PATH = '../data/pickle/' if not IN_COLAB else '/content/drive/MyDrive/STUDIA/INZ/data/pickle'\n",
        "TRAIN_PICKLE = 'train_dataset.pkl'\n",
        "TEST_PICKLE = 'test_dataset.pkl'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "qHByKsbscwoS"
      },
      "outputs": [],
      "source": [
        "train_pickle_path = os.path.join(PICKLE_PATH, TRAIN_PICKLE)\n",
        "test_pickle_path = os.path.join(PICKLE_PATH, TEST_PICKLE)\n",
        "\n",
        "if TRAIN_PICKLE not in os.listdir(PICKLE_PATH) and TEST_PICKLE not in os.listdir(PICKLE_PATH):\n",
        "    ids_dataset = dataset.DatasetIDS2018(csv_file_name='../data/raw/small_merge_data.csv')\n",
        "    train = ids_dataset.get_data_by_labels(labels=train_labels)\n",
        "    train_dataset = parse_dataset(dataset=train, columns_to_drop=columns_to_drop, fixed_type=float, labels_emb=labels_emb, labels_column_name='Label')\n",
        "    train_dataset.to_pickle(train_pickle_path)\n",
        "    test = ids_dataset.get_data_by_labels(labels=test_labels)\n",
        "    test_dataset = parse_dataset(dataset=test, columns_to_drop=columns_to_drop, fixed_type=float, labels_emb=labels_emb, labels_column_name='Label')\n",
        "    test_dataset.to_pickle(test_pickle_path)\n",
        "else:\n",
        "    train_dataset = pd.read_pickle(train_pickle_path)\n",
        "    test_dataset = pd.read_pickle(test_pickle_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_labels = torch.from_numpy(test_dataset.values[:, -1].astype(float)).float()\n",
        "all_labels"
      ],
      "metadata": {
        "id": "EqCC6dpDUeIX",
        "outputId": "6c4032c9-cffb-4ac2-e1e1-545bb6166882",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.,  0.,  0.,  ...,  0., 14., 14.])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(all_labels)"
      ],
      "metadata": {
        "id": "Z7u3oG_Obrf3",
        "outputId": "87de44c9-0129-4be7-b827-484f7028548d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6414673"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "-Tij4op5cwoT"
      },
      "outputs": [],
      "source": [
        "test_dataset = data.TensorDataset(torch.from_numpy(test_dataset.values).float(),torch.from_numpy(test_dataset.values[:,-1].astype(float)).float())\n",
        "train_dataset = data.TensorDataset(torch.from_numpy(train_dataset.values).float(),torch.from_numpy(train_dataset.values[:,-1].astype(float)).float())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_dataset)"
      ],
      "metadata": {
        "id": "q4ZfyxHBR6_J",
        "outputId": "35f5381c-fdc1-48ce-e4d1-958c75537f8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6414673"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "6GWKmk2wcwoT"
      },
      "outputs": [],
      "source": [
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Train len: {len(train_dataset)}\\tTrain len % batch_size = {len(train_dataset) % batch_size}\\nTest len: {len(test_dataset)}\\tTest len % batch_size = {len(test_dataset) % batch_size}')"
      ],
      "metadata": {
        "id": "AD48turX5u55",
        "outputId": "e85837a0-09ed-4ffa-f3a6-ca1b89e36c75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train len: 1869581\tTrain len % batch_size = 13\n",
            "Test len: 6414673\tTest len % batch_size = 17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Ps2bgF0GcwoT"
      },
      "outputs": [],
      "source": [
        "train_data_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_data_loader = data.DataLoader(test_dataset, batch_size=1, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCswOD-9cwoT",
        "outputId": "c2300329-e3d8-41cf-875b-c544296c337f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[6.0000e+00, 1.2883e+04, 2.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
              "          3.0000e+00],\n",
              "         [6.0000e+00, 4.8300e+02, 2.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
              "          1.0000e+00],\n",
              "         [6.0000e+00, 1.2561e+05, 3.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
              "          7.0000e+00],\n",
              "         ...,\n",
              "         [6.0000e+00, 1.3680e+03, 3.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
              "          3.0000e+00],\n",
              "         [6.0000e+00, 1.5440e+03, 2.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
              "          3.0000e+00],\n",
              "         [6.0000e+00, 6.3924e+04, 3.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
              "          7.0000e+00]]),\n",
              " tensor([ 3.,  1.,  7.,  3.,  9.,  9.,  7.,  3.,  7.,  3.,  3.,  1.,  1.,  3.,\n",
              "          3.,  3.,  1.,  1., 10.,  9.,  1.,  1.,  3.,  3.,  1.,  7.,  7.,  7.,\n",
              "          3.,  3.,  3.,  7.])]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "next(iter(train_data_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NPEA4lwcwoT",
        "outputId": "78c3efdf-ea3b-443b-a356-2893e16718a1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[ 6.0000e+00,  5.3839e+07,  2.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "           3.7148e-02,  5.3839e+07,  0.0000e+00,  5.3839e+07,  5.3839e+07,\n",
              "           5.3839e+07,  5.3839e+07,  0.0000e+00,  5.3839e+07,  5.3839e+07,\n",
              "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  4.0000e+01,\n",
              "           0.0000e+00,  3.7148e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "           0.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "           0.0000e+00,  2.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "           2.7900e+02, -1.0000e+00,  0.0000e+00,  2.0000e+01,  0.0000e+00,\n",
              "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "           0.0000e+00,  0.0000e+00,  0.0000e+00]]),\n",
              " tensor([0.])]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "next(iter(test_data_loader))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9SAFgu2cwoU"
      },
      "source": [
        "### **Variables**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "zZcaliZNcwoU"
      },
      "outputs": [],
      "source": [
        "word_vector_size = 100\n",
        "input_dim = len(train_dataset[0][0])\n",
        "output_dim = len(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Si6ZqMAcwoU",
        "outputId": "36e4c97e-df6e-4bd1-fce8-0eae3b64098a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input dim: 78\n",
            "Output dim: 16\n"
          ]
        }
      ],
      "source": [
        "print(f'Input dim: {input_dim}\\nOutput dim: {output_dim}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVWq9-mxcwoU"
      },
      "source": [
        "## **Word2Vec**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "45Q4XmYCcwoU"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import gensim.downloader as api\n",
        "from gensim.models.word2vec import Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "JAQzign2cwoU"
      },
      "outputs": [],
      "source": [
        "corpus = []\n",
        "for label in labels:\n",
        "    words = [word.translate(str.maketrans('', '', string.punctuation)) for word in label.split()]\n",
        "    corpus.append(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "HbxhcT0fcwoU"
      },
      "outputs": [],
      "source": [
        "gensim_model = Word2Vec(corpus, vector_size=word_vector_size, min_count=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "PpEtg4XMcwoU"
      },
      "outputs": [],
      "source": [
        "class LabelsEmbeddings():\n",
        "    def __init__(self, gensim_model: Word2Vec):\n",
        "        self.model = gensim_model\n",
        "\n",
        "    def fix_vectors_sizes(self, vectors: list) -> list:\n",
        "        fixed_vectors = []\n",
        "        max_size = max([len(v) for v in vectors])\n",
        "        for vector in vectors:\n",
        "            size_diff = max_size - len(vector)\n",
        "            vector.extend([[0] * word_vector_size] * size_diff)\n",
        "            fixed_vectors.append(vector)\n",
        "        return fixed_vectors\n",
        "\n",
        "    def generate_vectors(self, labels: dict):\n",
        "        vectors = []\n",
        "        for label in labels:\n",
        "            description_vector = []\n",
        "            words = [word.translate(str.maketrans('', '', string.punctuation)) for word in label.split()]\n",
        "            for word in words:\n",
        "                if word in self.model.wv.index_to_key:\n",
        "                    description_vector.append(self.model.wv[word])\n",
        "                else:\n",
        "                    description_vector.append(word_vector_size * [0])\n",
        "            vectors.append(description_vector)\n",
        "        return self.fix_vectors_sizes(vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "c_yD01CKcwoU"
      },
      "outputs": [],
      "source": [
        "labels_embeddings = LabelsEmbeddings(gensim_model=gensim_model)\n",
        "labels_vectors = labels_embeddings.generate_vectors(labels=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPaNLkBAcwoU",
        "outputId": "2d019822-953e-48a1-995b-440da5af5c55"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "[len(v) for v in labels_vectors]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "n9URX08ecwoV"
      },
      "outputs": [],
      "source": [
        "for vector in labels_vectors:\n",
        "    for word in vector:\n",
        "        assert len(word) == word_vector_size\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whiZad7YcwoV"
      },
      "source": [
        "### **Map layer**\n",
        "Last layer of the model should be a map between incident embeddings and labels embeddings - we want to map given input data to the most corresponding Word2Vec label vector. To do this we have to initialize weights of this layer and freeze them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "2qwLIzvLcwoV"
      },
      "outputs": [],
      "source": [
        "def map_layer_init(w2c_vectors: list) -> torch.Tensor:\n",
        "    vectors = np.asarray(w2c_vectors, dtype=float)\n",
        "    vectors = torch.from_numpy(vectors)\n",
        "    return vectors[:, -1, :].to(torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOpErBdacwoV",
        "outputId": "7c84b795-5255-4577-d982-b0d82952d3b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size: torch.Size([16, 100])\n",
            "Type: torch.float32\n"
          ]
        }
      ],
      "source": [
        "map_layer = map_layer_init(labels_vectors)\n",
        "print(f\"Size: {map_layer.size()}\\nType: {map_layer.dtype}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYVjbErKcwoV"
      },
      "source": [
        "## **Neural Network for Network Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "k9e-qYIocwoV"
      },
      "outputs": [],
      "source": [
        "class NetNet(nn.Module):\n",
        "    def __init__(self, input_dim: int, words_embeddings_dim: int, output_dim: int, labels_vectors: torch.Tensor):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(input_dim, 512)\n",
        "        self.bn1 = nn.BatchNorm1d(512)\n",
        "        self.linear2 = nn.Linear(512, 256)\n",
        "        self.bn2 = nn.BatchNorm1d(256)\n",
        "        self.linear3 = nn.Linear(256, words_embeddings_dim)\n",
        "        self.bn3 = nn.BatchNorm1d(words_embeddings_dim)\n",
        "        self.linear4 = nn.Linear(words_embeddings_dim, output_dim)\n",
        "        self.ReLU = nn.LeakyReLU()\n",
        "\n",
        "        # weight initialize\n",
        "        self.linear4.weight.data = map_layer_init(w2c_vectors=labels_vectors)\n",
        "        # freeze layer weights\n",
        "        self.linear4.weight.requires_grad = False\n",
        "\n",
        "        self.prev_x = torch.empty([])\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.ReLU(self.linear1(x))\n",
        "        x = self.ReLU(self.linear2(x))\n",
        "        x = self.ReLU(self.linear3(x))\n",
        "        x = self.linear4(x)\n",
        "        x = F.softmax(x, dim=1)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "nUzM-25ecwoV"
      },
      "outputs": [],
      "source": [
        "def print_model_layer_gradients(model):\n",
        "    for name, layer in model.named_modules():\n",
        "        if len(list(layer.named_modules())) == 1 and name != 'ReLU':\n",
        "            print(f\"Layer: {name}\\nGradients: {layer.weight.grad}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9o4-9qdcwoV"
      },
      "source": [
        "### **Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "vQ9L3A-RcwoV"
      },
      "outputs": [],
      "source": [
        "def train_model(model: NetNet, epochs: int, data_loader: data.DataLoader, loss_fn: nn.MSELoss, additional_eps=1e-06):\n",
        "        model.train()\n",
        "        for epoch in range(epochs):\n",
        "            for inputs, labels in data_loader:\n",
        "                if not torch.isfinite(inputs).all():\n",
        "                      inputs = torch.nan_to_num(inputs) # removing nan values\n",
        "                if additional_eps:\n",
        "                      inputs = torch.add(inputs, additional_eps)\n",
        "\n",
        "                labels = labels.to(device)\n",
        "                inputs = inputs.to(device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = loss_fn(outputs, labels.to(torch.long))\n",
        "                loss.backward(retain_graph=True)\n",
        "                model.optim.step()\n",
        "                model.optim.zero_grad()\n",
        "\n",
        "            #print(f'Labels: {labels}\\nOutputs: {outputs}')\n",
        "            print(f\"Epoch: {epoch}, loss: {loss.item():.3}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9_rhgaMcwoW",
        "outputId": "bbf30a69-25d9-4168-8698-a33f4a5ce13a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NetNet(\n",
              "  (linear1): Linear(in_features=78, out_features=512, bias=True)\n",
              "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
              "  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (linear3): Linear(in_features=256, out_features=100, bias=True)\n",
              "  (bn3): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (linear4): Linear(in_features=100, out_features=16, bias=True)\n",
              "  (ReLU): LeakyReLU(negative_slope=0.01)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "model = NetNet(\n",
        "    input_dim=input_dim,\n",
        "    words_embeddings_dim=word_vector_size,\n",
        "    output_dim=output_dim,\n",
        "    labels_vectors=labels_vectors\n",
        ")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "o-UcW2OFcwoW"
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-06"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qvhte_qLcwoc",
        "outputId": "e56d38c2-aa47-48ca-f457-34b49d40e2eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, loss: -0.0777\n",
            "Epoch: 1, loss: -0.0861\n",
            "Epoch: 2, loss: -0.0876\n",
            "Epoch: 3, loss: -0.0931\n",
            "Epoch: 4, loss: -0.0956\n"
          ]
        }
      ],
      "source": [
        "loss_fn = nn.NLLLoss()\n",
        "model.optim = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "train_model(model=model, epochs=5, data_loader=train_data_loader, loss_fn=loss_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCUA5hYxcwod"
      },
      "source": [
        "### **Removing last layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2PBAwZvcwod",
        "outputId": "18d02d2e-7b70-4520-acc5-ae49395a24a4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=78, out_features=512, bias=True)\n",
              "  (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (2): Linear(in_features=512, out_features=256, bias=True)\n",
              "  (3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (4): Linear(in_features=256, out_features=100, bias=True)\n",
              "  (5): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (6): LeakyReLU(negative_slope=0.01)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "zsl_model = nn.Sequential(*(list(model.children())[:6] + list(model.children())[7:]))\n",
        "zsl_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60Zd6SHtcwod",
        "outputId": "8335cf57-8d93-4c80-dff3-2a8f08b9d206"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NetNet(\n",
            "  (linear1): Linear(in_features=78, out_features=512, bias=True)\n",
            "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (linear3): Linear(in_features=256, out_features=100, bias=True)\n",
            "  (bn3): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (linear4): Linear(in_features=100, out_features=16, bias=True)\n",
            "  (ReLU): LeakyReLU(negative_slope=0.01)\n",
            ")\n",
            "Sequential(\n",
            "  (0): Linear(in_features=78, out_features=512, bias=True)\n",
            "  (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (2): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (4): Linear(in_features=256, out_features=100, bias=True)\n",
            "  (5): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (6): LeakyReLU(negative_slope=0.01)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)\n",
        "print(zsl_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRfCWx9ccwod"
      },
      "source": [
        "### **Evaluation**\n",
        "For this step we will calculate euclidean distance and find the vector which is the closest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "_9S0XhTecwod"
      },
      "outputs": [],
      "source": [
        "def find_closest_vector(vector: torch.Tensor, labels_vectors: torch.Tensor):\n",
        "    min_dist = float('inf')\n",
        "    min_dist_label = float('inf')\n",
        "    pdist = torch.nn.PairwiseDistance(p=2)\n",
        "    for label, label_vector in enumerate(labels_vectors, start=0):\n",
        "        #dist = torch.cdist(label_vector, vector, p=2)\n",
        "        #dist = dist.squeeze(dim=0)\n",
        "        dist = pdist(label_vector, vector)\n",
        "        if dist < min_dist:\n",
        "            min_dist = dist\n",
        "            min_dist_label = float(label)\n",
        "    return min_dist_label, min_dist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "5ZQpHxpPcwod"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model: NetNet, data_loader: data.DataLoader, labels_vectors: torch.Tensor):\n",
        "        model.eval()\n",
        "        true_predictions, predicitons_amount = 0., 0.\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in data_loader:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                labels_vectors = labels_vectors.to(device)\n",
        "\n",
        "                pred_input = model(inputs)\n",
        "                pred_label, dist = find_closest_vector(vector=pred_input[0], labels_vectors=labels_vectors)\n",
        "                #predictions = predictions.squeeze(dim=1)\n",
        "                true_predictions += int(pred_label == labels[0])\n",
        "                predicitons_amount += 1\n",
        "\n",
        "            accuracy = 100.0 * true_predictions / predicitons_amount\n",
        "\n",
        "        print(f\"Accuracy of the model: {accuracy:4.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XiYNwGXUcwod",
        "outputId": "16a4b506-05cc-4c87-e12a-5b4fbfe14fc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the model: 70.12%\n"
          ]
        }
      ],
      "source": [
        "evaluate_model(zsl_model, test_data_loader, map_layer)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}